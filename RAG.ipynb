{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)\n",
    "\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "# from langchain.llms import OpenAI\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "## ^^ This cell will be timed to see how long the conversation embedding takes\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['NVIDIA_API_KEY'] = os.getenv('NVIDIA_API_KEY')\n",
    "os.environ['ADOBE_CLIENT_ID'] = os.getenv('ADOBE_CLIENT_ID')\n",
    "os.environ['ADOBE_CLIENT_SECRET'] = os.getenv('ADOBE_CLIENT_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms import llms \n",
    "from llms import set_api_key\n",
    "set_api_key('NVIDIA_API_KEY', os.environ['NVIDIA_API_KEY'])\n",
    "llm = llms.nvai_mixtral_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adobe_pdf_services import AdobePDFParser\n",
    "from adobe_pdf_services import AdobePDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = AdobePDFParser(\n",
    "    client_id=os.environ['ADOBE_CLIENT_ID'],\n",
    "    client_secret=os.environ['ADOBE_CLIENT_SECRET'],\n",
    "    mode='chunks', # mode in [\"json\", \"chunks\", \"data\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIPS-2017-attention-is-all-you-need-Paper.pdf\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.path.join('data')\n",
    "file_lst = sorted(glob.glob(os.path.join(data_dir, '*.pdf')))\n",
    "for item in file_lst:\n",
    "    print(item.split('/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NIPS-2017-attention-is-all-you-need-Paper.pdf\n"
     ]
    }
   ],
   "source": [
    "for item in file_lst:\n",
    "    file_name = item.split('/')[-1]\n",
    "    print(file_name)\n",
    "    # if os.path.exists(os.path.join(target_dir, file_name)):\n",
    "    #     continue\n",
    "    # shutil.copy(item, target_dir)\n",
    "    loader = AdobePDFLoader(item, parser=parser)\n",
    "    chunks = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Attention Is All You Need </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Ashish Vaswani∗ Google Brain avaswani@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Noam Shazeer</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Google Brain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">noam@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Niki Parmar∗ Google Research nikip@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Jakob Uszkoreit∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Google Research usz@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Llion Jones</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Google Research </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">llion@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">† </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Aidan N. Gomez</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">University of Toronto </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">aidan@cs.toronto.edu </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Łukasz Kaiser</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Google Brain </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lukaszkaiser@google.com </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">∗ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">‡ </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Illia Polosukhin</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">illia.polosukhin@gmail.com</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAttention Is All You Need \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAshish Vaswani∗ Google Brain avaswani@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mNoam Shazeer\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mGoogle Brain \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mnoam@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mNiki Parmar∗ Google Research nikip@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mJakob Uszkoreit∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mGoogle Research usz@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLlion Jones\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mGoogle Research \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mllion@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m† \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAidan N. Gomez\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mUniversity of Toronto \u001b[0m\n",
       "\u001b[1;38;2;118;185;0maidan@cs.toronto.edu \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mŁukasz Kaiser\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mGoogle Brain \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlukaszkaiser@google.com \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m∗ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m‡ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mIllia Polosukhin\u001b[0m\n",
       "\u001b[1;38;2;118;185;0millia.polosukhin@gmail.com\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">include an encoder and a decoder. The best performing models also connect the encoder and decoder through an </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">these models to be superior in quality while being more parallelizable and requiring significantly less time to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">train. Our model achieves </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU on the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German translation task, improving over the existing</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">best results, including ensembles, by over </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> BLEU. On the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-French translation task, our model </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">establishes a new single-model state-of-the-art BLEU score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> after training for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> days on eight GPUs, a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">small fraction of the training costs of the best models from the literature.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minclude an encoder and a decoder. The best performing models also connect the encoder and decoder through an \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mattention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthese models to be superior in quality while being more parallelizable and requiring significantly less time to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtrain. Our model achieves \u001b[0m\u001b[1;36m28.4\u001b[0m\u001b[1;38;2;118;185;0m BLEU on the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German translation task, improving over the existing\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbest results, including ensembles, by over \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m BLEU. On the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-French translation task, our model \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mestablishes a new single-model state-of-the-art BLEU score of \u001b[0m\u001b[1;36m41.0\u001b[0m\u001b[1;38;2;118;185;0m after training for \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m days on eight GPUs, a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msmall fraction of the training costs of the best models from the literature.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Recurrent neural networks, long short-term memory [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] and gated recurrent [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] neural networks in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">particular, have been firmly established as state of the art approaches in sequence modeling and transduction </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">problems such as language modeling and machine translation [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. Numerous efforts have since </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">continued to push the boundaries of recurrent language models and encoder-decoder architectures [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">attention and the parameter-free position representation and became the other person involved in nearly every </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">our research. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">†Work performed while at Google Brain. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">‡Work performed while at Google Research. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">31st Conference on Neural Information Processing Systems (NIPS </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">), Long Beach, CA, USA. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Recurrent models typically factor computation along the symbol positions of the input and output sequences. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">the previous hidden state ht−</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> and the input for position t. This inherently sequential nature precludes </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">limit batching across examples. Recent work has achieved significant improvements in computational efficiency </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">through factorization tricks [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] and conditional computation [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], while also improving model performance </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">in case of the latter. The fundamental constraint of sequential computation, however, remains. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. In all but a few cases [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], however, such attention mechanisms are used in conjunction with a</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">recurrent network. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">an attention mechanism to draw global dependencies between input and output. The Transformer allows for </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">significantly more parallelization and can reach a new state of the art in translation quality after being trained </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for as little as twelve hours on eight P100 GPUs.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mRecurrent neural networks, long short-term memory \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m and gated recurrent \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m neural networks in \u001b[0m\n",
       "\u001b[1;39mparticular, have been firmly established as state of the art approaches in sequence modeling and transduction \u001b[0m\n",
       "\u001b[1;39mproblems such as language modeling and machine translation \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. Numerous efforts have since \u001b[0m\n",
       "\u001b[1;39mcontinued to push the boundaries of recurrent language models and encoder-decoder architectures \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;39m, \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39m∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the \u001b[0m\n",
       "\u001b[1;39meffort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has \u001b[0m\n",
       "\u001b[1;39mbeen crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head \u001b[0m\n",
       "\u001b[1;39mattention and the parameter-free position representation and became the other person involved in nearly every \u001b[0m\n",
       "\u001b[1;39mdetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and \u001b[0m\n",
       "\u001b[1;39mtensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and \u001b[0m\n",
       "\u001b[1;39mefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and \u001b[0m\n",
       "\u001b[1;39mimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating \u001b[0m\n",
       "\u001b[1;39mour research. \u001b[0m\n",
       "\u001b[1;39m†Work performed while at Google Brain. \u001b[0m\n",
       "\u001b[1;39m‡Work performed while at Google Research. \u001b[0m\n",
       "\u001b[1;39m31st Conference on Neural Information Processing Systems \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mNIPS \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m, Long Beach, CA, USA. \u001b[0m\n",
       "\u001b[1;39mRecurrent models typically factor computation along the symbol positions of the input and output sequences. \u001b[0m\n",
       "\u001b[1;39mAligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of\u001b[0m\n",
       "\u001b[1;39mthe previous hidden state ht−\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m and the input for position t. This inherently sequential nature precludes \u001b[0m\n",
       "\u001b[1;39mparallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints \u001b[0m\n",
       "\u001b[1;39mlimit batching across examples. Recent work has achieved significant improvements in computational efficiency \u001b[0m\n",
       "\u001b[1;39mthrough factorization tricks \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m and conditional computation \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, while also improving model performance \u001b[0m\n",
       "\u001b[1;39min case of the latter. The fundamental constraint of sequential computation, however, remains. \u001b[0m\n",
       "\u001b[1;39mAttention mechanisms have become an integral part of compelling sequence modeling and transduction models in \u001b[0m\n",
       "\u001b[1;39mvarious tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences \u001b[0m\n",
       "\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. In all but a few cases \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, however, such attention mechanisms are used in conjunction with a\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrecurrent network. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mIn this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on \u001b[0m\n",
       "\u001b[1;38;2;118;185;0man attention mechanism to draw global dependencies between input and output. The Transformer allows for \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msignificantly more parallelization and can reach a new state of the art in translation quality after being trained \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor as little as twelve hours on eight P100 GPUs.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], ByteNet </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">[(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] and ConvS2S [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], all of which use convolutional neural networks as basic building block, computing </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">hidden representations in parallel for all input and output positions. In these models, the number of operations </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">required to relate signals from two arbitrary input or output positions grows in the distance between positions, </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">distant positions [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. In the Transformer this is reduced to a constant number of operations, albeit at the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Multi-Head Attention as described in section (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">task-independent sentence representations [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">have been shown to perform well on simple-language question answering and language modeling tasks [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">self-attention to compute representations of its input and output without using sequence-aligned RNNs or </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">advantages over models such as [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] and [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">].</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, ByteNet \u001b[0m\n",
       "\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m and ConvS2S \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, all of which use convolutional neural networks as basic building block, computing \u001b[0m\n",
       "\u001b[1;39mhidden representations in parallel for all input and output positions. In these models, the number of operations \u001b[0m\n",
       "\u001b[1;39mrequired to relate signals from two arbitrary input or output positions grows in the distance between positions, \u001b[0m\n",
       "\u001b[1;39mlinearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between \u001b[0m\n",
       "\u001b[1;39mdistant positions \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. In the Transformer this is reduced to a constant number of operations, albeit at the \u001b[0m\n",
       "\u001b[1;39mcost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with \u001b[0m\n",
       "\u001b[1;39mMulti-Head Attention as described in section \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single\u001b[0m\n",
       "\u001b[1;39msequence in order to compute a representation of the sequence. Self-attention has been used successfully in a \u001b[0m\n",
       "\u001b[1;39mvariety of tasks including reading comprehension, abstractive summarization, textual entailment and learning \u001b[0m\n",
       "\u001b[1;39mtask-independent sentence representations \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and \u001b[0m\n",
       "\u001b[1;39mhave been shown to perform well on simple-language question answering and language modeling tasks \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mTo the best of our knowledge, however, the Transformer is the first transduction model relying entirely on \u001b[0m\n",
       "\u001b[1;39mself-attention to compute representations of its input and output without using sequence-aligned RNNs or \u001b[0m\n",
       "\u001b[1;39mconvolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its \u001b[0m\n",
       "\u001b[1;39madvantages over models such as \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m and \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Most competitive neural sequence transduction models have an encoder-decoder structure [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Here, the encoder maps an input sequence of symbol representations (x1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, xn) to a sequence of continuous </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">representations z = (z1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, zn). Given z, the decoder then generates an output sequence (y1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, ym) of symbols </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">one element at a time. At each step the model is auto-regressive [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], consuming the previously generated </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">symbols as additional input when generating the next. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">layers for both the encoder and decoder, shown in the left and right halves of Figure (&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, respectively.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mMost competitive neural sequence transduction models have an encoder-decoder structure \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mHere, the encoder maps an input sequence of symbol representations \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mx1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;39m, xn\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m to a sequence of continuous \u001b[0m\n",
       "\u001b[1;39mrepresentations z = \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mz1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;39m, zn\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m. Given z, the decoder then generates an output sequence \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39my1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;39m, ym\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m of symbols \u001b[0m\n",
       "\u001b[1;39mone element at a time. At each step the model is auto-regressive \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, consuming the previously generated \u001b[0m\n",
       "\u001b[1;39msymbols as additional input when generating the next. \u001b[0m\n",
       "\u001b[1;39mThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected \u001b[0m\n",
       "\u001b[1;39mlayers for both the encoder and decoder, shown in the left and right halves of Figure \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m, respectively.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Encoder: The encoder is composed of a stack of N = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> identical layers. Each layer has two sub-layers. The first is </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">a multi-head self-attention mechanism, and the second is a simple, position-</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart0.png}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: The Transformer -model architecture. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">wise fully connected feed-forward network. We employ a residual connection [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] around each of the two </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">sub-layers, followed by layer normalization [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. That is, the output of each sub-layer is </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LayerNorm</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(x + </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sublayer</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(x)), where </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sublayer</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(x) is the function implemented by the sub-layer itself. To facilitate these residual </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Decoder: The decoder is also composed of a stack of N = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> identical layers. In addition to the two sub-layers in </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">position, ensures that the predictions for position i can depend only on the known outputs at positions less than </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mEncoder: The encoder is composed of a stack of N = \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m identical layers. Each layer has two sub-layers. The first is \u001b[0m\n",
       "\u001b[1;38;2;118;185;0ma multi-head self-attention mechanism, and the second is a simple, position-\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart0.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFigure \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m: The Transformer -model architecture. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwise fully connected feed-forward network. We employ a residual connection \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m around each of the two \u001b[0m\n",
       "\u001b[1;39msub-layers, followed by layer normalization \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. That is, the output of each sub-layer is \u001b[0m\u001b[1;35mLayerNorm\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mx + \u001b[0m\n",
       "\u001b[1;35mSublayer\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mx\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, where \u001b[0m\u001b[1;35mSublayer\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mx\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m is the function implemented by the sub-layer itself. To facilitate these residual \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconnections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = \u001b[0m\n",
       "\u001b[1;36m512\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDecoder: The decoder is also composed of a stack of N = \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m identical layers. In addition to the two sub-layers in \u001b[0m\n",
       "\u001b[1;38;2;118;185;0meach encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mby layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mattending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mposition, ensures that the predictions for position i can depend only on the known outputs at positions less than \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">An attention function can be described as mapping a query and a set of key-value pairs to an output, where the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mAn attention function can be described as mapping a query and a set of key-value pairs to an output, where the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mquery, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mweight assigned to each value is computed by a compatibility function of the query with the corresponding key.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We call our particular attention </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Scaled Dot-Product Attention\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> (Figure (&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">). The input consists of queries and </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">keys of dimension dk, and values of dimension dv. We compute the dot products of the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Scaled Dot-Product Attention </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart1.png}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Multi-Head Attention </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart2.png}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Figure </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">running in parallel. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart3.png}</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">√ query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart4.png}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Attention</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(Q, K, V ) = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">softmax</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">( QKT √ dk )V </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">) </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart5.png}</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">k </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">√d. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">The two most commonly used attention functions are additive attention [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], and dot-product (multiplicative) </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">attention. Dot-product attention is identical to our algorithm, except for the scaling factor of Additive attention</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">computes the compatibility function using a feed-forward network with a single hidden layer. While the two are </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">it can be implemented using highly optimized matrix multiplication code. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart6.png}</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">√</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">attention without scaling for larger values of dk [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. We suspect that for large values of dk, the dot products</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> (&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;). </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To counteract this effect, we scale the dot products by dk</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWe call our particular attention \u001b[0m\u001b[32m\"Scaled Dot-Product Attention\"\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mFigure \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m. The input consists of queries and \u001b[0m\n",
       "\u001b[1;39mkeys of dimension dk, and values of dimension dv. We compute the dot products of the \u001b[0m\n",
       "\u001b[1;39mScaled Dot-Product Attention \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart1.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;39mMulti-Head Attention \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart2.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;39mFigure \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m: \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mleft\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m Scaled Dot-Product Attention. \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mright\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m Multi-Head Attention consists of several attention layers \u001b[0m\n",
       "\u001b[1;39mrunning in parallel. \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart3.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;39m√ query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values. \u001b[0m\n",
       "\u001b[1;39mIn practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q.\u001b[0m\n",
       "\u001b[1;39mThe keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart4.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;35mAttention\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mQ, K, V \u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m = \u001b[0m\u001b[1;35msoftmax\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m QKT √ dk \u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39mV \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart5.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m\n",
       "\u001b[1;39mk \u001b[0m\n",
       "\u001b[1;39m√d. \u001b[0m\n",
       "\u001b[1;39mThe two most commonly used attention functions are additive attention \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, and dot-product \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mmultiplicative\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39mattention. Dot-product attention is identical to our algorithm, except for the scaling factor of Additive attention\u001b[0m\n",
       "\u001b[1;39mcomputes the compatibility function using a feed-forward network with a single hidden layer. While the two are \u001b[0m\n",
       "\u001b[1;39msimilar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since\u001b[0m\n",
       "\u001b[1;39mit can be implemented using highly optimized matrix multiplication code. \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart6.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;36m1\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39m√\u001b[0m\n",
       "\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product \u001b[0m\n",
       "\u001b[1;39mattention without scaling for larger values of dk \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. We suspect that for large values of dk, the dot products\u001b[0m\n",
       "\u001b[1;39mgrow large in magnitude, pushing the softmax function into regions where it has extremely small gradients \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTo counteract this effect, we scale the dot products by dk\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">beneficial to linearly project the queries, keys and values h times with different, learned linear projections to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">perform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">again projected, resulting in the final values, as depicted in Figure (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Multi-head attention allows the model to jointly attend to information from different representation subspaces at </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">different positions. With a single attention head, averaging inhibits this. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">4To illustrate why the dot products get large, assume that the components of q and k are independent random </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">variables with mean </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> and variance </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Then their dot product, q · k = </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart7.png}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">k </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i</span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">di</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> qk, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">has mean </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> and variance dk. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart8.png}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MultiHead</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(Q, K, V ) = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Concat</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(head1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, headh)W O </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart9.png}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">where headi = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Attention</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(QW Q i , KW K i ,V W V i ) </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart10.png}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Q </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dmodel×dk </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">K </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dmodel×dk </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">V </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">i</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dmodel×dv </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">W∈ R, W∈ R, W∈ R</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Where the projections are parameter matrices and W O ∈ Rhdv ×dmodel . </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In this work we employ h = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">with full dimensionality.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mInstead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeneficial to linearly project the queries, keys and values h times with different, learned linear projections to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperform the attention function in parallel, yielding dv-dimensional output values. These are concatenated and once \u001b[0m\n",
       "\u001b[1;38;2;118;185;0magain projected, resulting in the final values, as depicted in Figure \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMulti-head attention allows the model to jointly attend to information from different representation subspaces at \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdifferent positions. With a single attention head, averaging inhibits this. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m4To illustrate why the dot products get large, assume that the components of q and k are independent random \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvariables with mean \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m and variance \u001b[0m\n",
       "\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m. Then their dot product, q · k = \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart7.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mk \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi\u001b[0m\n",
       "\u001b[1;33mdi\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m qk, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhas mean \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m and variance dk. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart8.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;35mMultiHead\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mQ, K, V \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m = \u001b[0m\u001b[1;35mConcat\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mhead1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m, headh\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0mW O \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart9.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwhere headi = \u001b[0m\u001b[1;35mAttention\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mQW Q i , KW K i ,V W V i \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart10.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mQ \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdmodel×dk \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mK \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdmodel×dk \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mV \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mi\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdmodel×dv \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mW∈ R, W∈ R, W∈ R\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWhere the projections are parameter matrices and W O ∈ Rhdv ×dmodel . \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mIn this work we employ h = \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = \u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mDue to the reduced dimension of each head, the total computational cost is similar to that of single-head attention\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mwith full dimensionality.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer uses multi-head attention in three different ways: </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">•  In </span><span style=\"color: #008000; text-decoration-color: #008000\">\"encoder-decoder attention\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> layers, the queries come from the previous decoder layer, and the memory keys and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">values come from the output of the encoder. This allows every position in the decoder to attend over all positions </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">such as [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">•  The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">can attend to all positions in the previous layer of the encoder. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">•  Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">the decoder up to and including that position. We need to prevent leftward information flow in the decoder to </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure (&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Transformer uses multi-head attention in three different ways: \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m•  In \u001b[0m\u001b[32m\"encoder-decoder attention\"\u001b[0m\u001b[1;38;2;118;185;0m layers, the queries come from the previous decoder layer, and the memory keys and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvalues come from the output of the encoder. This allows every position in the decoder to attend over all positions \u001b[0m\n",
       "\u001b[1;38;2;118;185;0min the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msuch as \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39m•  The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come \u001b[0m\n",
       "\u001b[1;39mfrom the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder \u001b[0m\n",
       "\u001b[1;39mcan attend to all positions in the previous layer of the encoder. \u001b[0m\n",
       "\u001b[1;39m•  Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in\u001b[0m\n",
       "\u001b[1;39mthe decoder up to and including that position. We need to prevent leftward information flow in the decoder to \u001b[0m\n",
       "\u001b[1;39mpreserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[1;39msetting to −∞\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m all values in the input of the softmax which correspond to illegal connections. See Figure \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">feed-forward network, which is applied to each position separately and identically. This consists of two linear </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">transformations with a ReLU activation in between. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart11.png}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FFN</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(x) = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">max</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, xW1 + b1)W2 + b2 </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">While the linear transformations are the same across different positions, they use different parameters from layer </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">to layer. Another way of describing this is as two convolutions with kernel size </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. The dimensionality of input and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">output is dmodel = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, and the inner-layer has dimensionality dff = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfeed-forward network, which is applied to each position separately and identically. This consists of two linear \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtransformations with a ReLU activation in between. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart11.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;35mFFN\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mx\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m = \u001b[0m\u001b[1;35mmax\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m, xW1 + b1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0mW2 + b2 \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWhile the linear transformations are the same across different positions, they use different parameters from layer \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mto layer. Another way of describing this is as two convolutions with kernel size \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m. The dimensionality of input and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0moutput is dmodel = \u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;38;2;118;185;0m, and the inner-layer has dimensionality dff = \u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">between the two embedding layers and the pre-softmax √ linear transformation, similar to [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. In the embedding</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">layers, we multiply those weights by dmodel.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetween the two embedding layers and the pre-softmax √ linear transformation, similar to \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. In the embedding\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlayers, we multiply those weights by dmodel.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">this end, we add </span><span style=\"color: #008000; text-decoration-color: #008000\">\"positional encodings\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> to the input embeddings at the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">size of the neighborhood in restricted self-attention. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|    | Layer Type                  | Complexity per Layer     | Sequential Operations     | Maximum Path Length    </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|---:|:----------------------------|:-------------------------|:--------------------------|:-----------------------</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">-|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> | Self-Attention              | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n · d)               | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)                      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)                   </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> | Recurrent                   | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n · d2)                | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n)                      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n)                   </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> | Convolutional               | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(k · n · d2)            | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)                      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">logk</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n))             </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> | Self-Attention (restricted) | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(r · n · d)             | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)                      | </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(n/r)                 </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">|</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In this work, we use sine and cosine functions of different frequencies: </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">{figures/fileoutpart13.png}</span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PE</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(pos,2i) = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">sin</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(pos/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> 2i/dmodel ) </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PE</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(pos,2i+</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">) = </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">cos</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(pos/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> 2i/dmodel ) </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">a sinusoid. The wavelengths form a geometric progression from 2π to </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10000</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · 2π. We chose this function because we </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">k, P Epos+k can be represented as a linear function of PEpos. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">We also experimented with using learned positional embeddings [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] instead, and found that the two versions </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">produced nearly identical results (see Table (&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> row (E)). We chose the sinusoidal version because it may allow </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the model to extrapolate to sequence lengths longer than the ones encountered during training.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthis end, we add \u001b[0m\u001b[32m\"positional encodings\"\u001b[0m\u001b[1;38;2;118;185;0m to the input embeddings at the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mTable \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mtypes. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msize of the neighborhood in restricted self-attention. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|    | Layer Type                  | Complexity per Layer     | Sequential Operations     | Maximum Path Length    \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|---:|:----------------------------|:-------------------------|:--------------------------|:-----------------------\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m-|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|  \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;38;2;118;185;0m | Self-Attention              | \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn · d\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m               | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                      | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                   \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|  \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m | Recurrent                   | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn · d2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                      | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                   \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|  \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m | Convolutional               | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mk · n · d2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m            | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                      | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;35mlogk\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m             \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|  \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m | Self-Attention \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mrestricted\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mr · n · d\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m             | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                      | \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mn/r\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m                 \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m|\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0membeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mIn this work, we use sine and cosine functions of different frequencies: \u001b[0m\n",
       "\u001b[1;39m{\u001b[0m\u001b[1;39mfigures/fileoutpart13.png\u001b[0m\u001b[1;39m}\u001b[0m\n",
       "\u001b[1;35mPE\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mpos,2i\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m = \u001b[0m\u001b[1;35msin\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mpos/\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;39m 2i/dmodel \u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;35mPE\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mpos,2i+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m = \u001b[0m\u001b[1;35mcos\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mpos/\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;39m 2i/dmodel \u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39mwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to\u001b[0m\n",
       "\u001b[1;39ma sinusoid. The wavelengths form a geometric progression from 2π to \u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;39m · 2π. We chose this function because we \u001b[0m\n",
       "\u001b[1;39mhypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset \u001b[0m\n",
       "\u001b[1;39mk, P Epos+k can be represented as a linear function of PEpos. \u001b[0m\n",
       "\u001b[1;39mWe also experimented with using learned positional embeddings \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m instead, and found that the two versions \u001b[0m\n",
       "\u001b[1;39mproduced nearly identical results \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39msee Table \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m row \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mE\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. We chose the sinusoidal version because it may allow \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe model to extrapolate to sequence lengths longer than the ones encountered during training.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In this section we compare various aspects of self-attention layers to the recurrent and convolu-tional layers </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">commonly used for mapping one variable-length sequence of symbol representations (x1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, xn) to another sequence </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of equal length (z1, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">...</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, zn), with xi, zi ∈ Rd , such as a hidden layer in a typical sequence transduction encoder</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">or decoder. Motivating our use of self-attention we consider three desiderata. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">One is the total computational complexity per layer. Another is the amount of computation that can be parallelized,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">as measured by the minimum number of sequential operations required. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">between any combination of positions in the input and output sequences, the easier it is to learn long-range </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">dependencies [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. Hence we also compare the maximum path length between any two input and output positions in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">networks composed of the different layer types. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">As noted in Table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, a self-attention layer connects all positions with a constant number of sequentially </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">executed operations, whereas a recurrent layer requires </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(n) sequential operations. In terms of computational </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">representation dimensionality d, which is most often the case with sentence representations used by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">state-of-the-art models in machine translations, such as word-piece [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] and byte-pair [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">representations. To improve computational performance for tasks involving very long sequences, self-attention could</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">be restricted to considering only a neighborhood of size r in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">the input sequence centered around the respective output position. This would increase the maximum path length to </span>\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(n/r). We plan to investigate this approach further in future work. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">A single convolutional layer with kernel width k &lt; n does not connect all pairs of input and output positions. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Doing so requires a stack of </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(n/k) convolutional layers in the case of contiguous kernels, or </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">logk</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(n)) in the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">case of dilated convolutions [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], increasing the length of the longest paths between any two positions in the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">convolutions [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], however, decrease the complexity considerably, to </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">O</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(k · n · d + n · d2). Even with k = n, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">point-wise feed-forward layer, the approach we take in our model. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sentences.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mIn this section we compare various aspects of self-attention layers to the recurrent and convolu-tional layers \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcommonly used for mapping one variable-length sequence of symbol representations \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mx1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m, xn\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m to another sequence \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof equal length \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mz1, \u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;38;2;118;185;0m, zn\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, with xi, zi ∈ Rd , such as a hidden layer in a typical sequence transduction encoder\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mor decoder. Motivating our use of self-attention we consider three desiderata. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mOne is the total computational complexity per layer. Another is the amount of computation that can be parallelized,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mas measured by the minimum number of sequential operations required. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe third is the path length between long-range dependencies in the network. Learning long-range dependencies is a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mkey challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mis the length of the paths forward and backward signals have to traverse in the network. The shorter these paths \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbetween any combination of positions in the input and output sequences, the easier it is to learn long-range \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdependencies \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. Hence we also compare the maximum path length between any two input and output positions in \u001b[0m\n",
       "\u001b[1;39mnetworks composed of the different layer types. \u001b[0m\n",
       "\u001b[1;39mAs noted in Table \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m, a self-attention layer connects all positions with a constant number of sequentially \u001b[0m\n",
       "\u001b[1;39mexecuted operations, whereas a recurrent layer requires \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mn\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m sequential operations. In terms of computational \u001b[0m\n",
       "\u001b[1;39mcomplexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the \u001b[0m\n",
       "\u001b[1;39mrepresentation dimensionality d, which is most often the case with sentence representations used by \u001b[0m\n",
       "\u001b[1;39mstate-of-the-art models in machine translations, such as word-piece \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m and byte-pair \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39mrepresentations. To improve computational performance for tasks involving very long sequences, self-attention could\u001b[0m\n",
       "\u001b[1;39mbe restricted to considering only a neighborhood of size r in \u001b[0m\n",
       "\u001b[1;39mthe input sequence centered around the respective output position. This would increase the maximum path length to \u001b[0m\n",
       "\u001b[1;35mO\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mn/r\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m. We plan to investigate this approach further in future work. \u001b[0m\n",
       "\u001b[1;39mA single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. \u001b[0m\n",
       "\u001b[1;39mDoing so requires a stack of \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mn/k\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m convolutional layers in the case of contiguous kernels, or \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;35mlogk\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mn\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m in the \u001b[0m\n",
       "\u001b[1;39mcase of dilated convolutions \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, increasing the length of the longest paths between any two positions in the \u001b[0m\n",
       "\u001b[1;39mnetwork. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable \u001b[0m\n",
       "\u001b[1;39mconvolutions \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, however, decrease the complexity considerably, to \u001b[0m\u001b[1;35mO\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mk · n · d + n · d2\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m. Even with k = n, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mhowever, the complexity of a separable convolution is equal to the combination of a self-attention layer and a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpoint-wise feed-forward layer, the approach we take in our model. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmodels and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mperform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msentences.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This section describes the training regime for our models.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThis section describes the training regime for our models.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We trained on the standard WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-German dataset consisting of about </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> million sentence pairs. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Sentences were encoded using byte-pair encoding [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">], which has a shared source-target vocabulary of about </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">37000</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">tokens. For English-French, we used the significantly larger WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> English-French dataset consisting of 36M </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">sentences and split tokens into a </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32000</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> word-piece vocabulary [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. Sentence pairs were batched together by </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">approximate sequence length. Each training batch contained a set of sentence pairs containing approximately </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">source tokens and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> target tokens.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWe trained on the standard WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-German dataset consisting of about \u001b[0m\u001b[1;36m4.5\u001b[0m\u001b[1;38;2;118;185;0m million sentence pairs. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mSentences were encoded using byte-pair encoding \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m, which has a shared source-target vocabulary of about \u001b[0m\u001b[1;36m37000\u001b[0m\n",
       "\u001b[1;39mtokens. For English-French, we used the significantly larger WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;39m English-French dataset consisting of 36M \u001b[0m\n",
       "\u001b[1;39msentences and split tokens into a \u001b[0m\u001b[1;36m32000\u001b[0m\u001b[1;39m word-piece vocabulary \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. Sentence pairs were batched together by \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mapproximate sequence length. Each training batch contained a set of sentence pairs containing approximately \u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msource tokens and \u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;38;2;118;185;0m target tokens.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We trained our models on one machine with </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> NVIDIA P100 GPUs. For our base models using the hyperparameters </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">described throughout the paper, each training step took about </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> seconds. We trained the base models for a total </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> steps or </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> hours. For our big models,(described on the bottom line of table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">), step time was </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">seconds. The big models were trained for </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">300</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> steps (</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> days).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWe trained our models on one machine with \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m NVIDIA P100 GPUs. For our base models using the hyperparameters \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdescribed throughout the paper, each training step took about \u001b[0m\u001b[1;36m0.4\u001b[0m\u001b[1;38;2;118;185;0m seconds. We trained the base models for a total \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mof \u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m steps or \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;38;2;118;185;0m hours. For our big models,\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mdescribed on the bottom line of table \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, step time was \u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mseconds. The big models were trained for \u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\u001b[1;36m000\u001b[0m\u001b[1;38;2;118;185;0m steps \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;38;2;118;185;0m days\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We used the Adam optimizer [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">] with β1 = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, β2 = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.98</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> and  = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">−</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> . We varied the learning rate over the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">course of training, according to the formula: </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{figures/fileoutpart14.png}</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">lrate = d−</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> model · </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">min</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(step_num−</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> , step_num · warmup_steps−</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">) </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4000</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWe used the Adam optimizer \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m with β1 = \u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;38;2;118;185;0m, β2 = \u001b[0m\u001b[1;36m0.98\u001b[0m\u001b[1;38;2;118;185;0m and  = \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m−\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m . We varied the learning rate over the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mcourse of training, according to the formula: \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0mfigures/fileoutpart14.png\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlrate = d−\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;38;2;118;185;0m model · \u001b[0m\u001b[1;35mmin\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mstep_num−\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;38;2;118;185;0m , step_num · warmup_steps−\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mit thereafter proportionally to the inverse square root of the step number. We used warmup_steps = \u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We employ three types of regularization during training: </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Residual Dropout We apply dropout [(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] to the output of each sub-layer, before it is added to the sub-layer </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">and English-to-French newstest2014 tests at a fraction of the training cost. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|                                 | Model     | BLEU     | Unnamed: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | Training Cost (FLOPs)     |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|:--------------------------------|:----------|:---------|:-------------|:--------------------------|</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| nan                             | EN-DE     | EN-FR    | EN-DE        | EN-FR                     |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| ByteNet [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]                    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.75</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | nan      | nan          | nan                       |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| Deep-Att + PosUnk [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]          | nan       | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | nan          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| GNMT + RL [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]                  | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">      | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">39.92</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1019</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| ConvS2S [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]                     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.46</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1018</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| MoE [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]                        | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.03</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.56</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1019</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| Deep-Att + PosUnk Ensemble [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">] | nan       | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">40.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | nan          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| GNMT + RL Ensemble [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]         | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.30</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1020</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1021</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| ConvS2S Ensemble [</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]            | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.36</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.29</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1019</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1021</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| Transformer (base model)        | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">      | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1018</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | nan                       |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| Transformer (big)               | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">      | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">     | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> · </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1019</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | nan                       |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Label Smoothing During training, we employed label smoothing of value ls = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]. This hurts perplexity, as</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the model learns to be more unsure, but improves accuracy and BLEU score.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mWe employ three types of regularization during training: \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mResidual Dropout We apply dropout \u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m to the output of each sub-layer, before it is added to the sub-layer \u001b[0m\n",
       "\u001b[1;39minput and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in \u001b[0m\n",
       "\u001b[1;39mboth the encoder and decoder stacks. For the base model, we use a rate of Pdrop = \u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mTable \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German\u001b[0m\n",
       "\u001b[1;39mand English-to-French newstest2014 tests at a fraction of the training cost. \u001b[0m\n",
       "\u001b[1;39m|                                 | Model     | BLEU     | Unnamed: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m   | Training Cost \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mFLOPs\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m     |\u001b[0m\n",
       "\u001b[1;39m|:--------------------------------|:----------|:---------|:-------------|:--------------------------|\u001b[0m\n",
       "\u001b[1;39m| nan                             | EN-DE     | EN-FR    | EN-DE        | EN-FR                     |\u001b[0m\n",
       "\u001b[1;39m| ByteNet \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m                    | \u001b[0m\u001b[1;36m23.75\u001b[0m\u001b[1;39m     | nan      | nan          | nan                       |\u001b[0m\n",
       "\u001b[1;39m| Deep-Att + PosUnk \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m          | nan       | \u001b[0m\u001b[1;36m39.2\u001b[0m\u001b[1;39m     | nan          | \u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| GNMT + RL \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m                  | \u001b[0m\u001b[1;36m24.6\u001b[0m\u001b[1;39m      | \u001b[0m\u001b[1;36m39.92\u001b[0m\u001b[1;39m    | \u001b[0m\u001b[1;36m2.3\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1019\u001b[0m\u001b[1;39m   | \u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| ConvS2S \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m                     | \u001b[0m\u001b[1;36m25.16\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m40.46\u001b[0m\u001b[1;39m    | \u001b[0m\u001b[1;36m9.6\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1018\u001b[0m\u001b[1;39m   | \u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| MoE \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m                        | \u001b[0m\u001b[1;36m26.03\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m40.56\u001b[0m\u001b[1;39m    | \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1019\u001b[0m\u001b[1;39m   | \u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| Deep-Att + PosUnk Ensemble \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m | nan       | \u001b[0m\u001b[1;36m40.4\u001b[0m\u001b[1;39m     | nan          | \u001b[0m\u001b[1;36m8.0\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| GNMT + RL Ensemble \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m         | \u001b[0m\u001b[1;36m26.30\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m41.16\u001b[0m\u001b[1;39m    | \u001b[0m\u001b[1;36m1.8\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1020\u001b[0m\u001b[1;39m   | \u001b[0m\u001b[1;36m1.1\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1021\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| ConvS2S Ensemble \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m            | \u001b[0m\u001b[1;36m26.36\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m41.29\u001b[0m\u001b[1;39m    | \u001b[0m\u001b[1;36m7.7\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1019\u001b[0m\u001b[1;39m   | \u001b[0m\u001b[1;36m1.2\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1021\u001b[0m\u001b[1;39m                |\u001b[0m\n",
       "\u001b[1;39m| Transformer \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mbase model\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m        | \u001b[0m\u001b[1;36m27.3\u001b[0m\u001b[1;39m      | \u001b[0m\u001b[1;36m38.1\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m3.3\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1018\u001b[0m\u001b[1;39m   | nan                       |\u001b[0m\n",
       "\u001b[1;39m| Transformer \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mbig\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m               | \u001b[0m\u001b[1;36m28.4\u001b[0m\u001b[1;39m      | \u001b[0m\u001b[1;36m41.0\u001b[0m\u001b[1;39m     | \u001b[0m\u001b[1;36m2.3\u001b[0m\u001b[1;39m · \u001b[0m\u001b[1;36m1019\u001b[0m\u001b[1;39m   | nan                       |\u001b[0m\n",
       "\u001b[1;39mLabel Smoothing During training, we employed label smoothing of value ls = \u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m. This hurts perplexity, as\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe model learns to be more unsure, but improves accuracy and BLEU score.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">On the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German translation task, the big transformer model (Transformer (big) in Table (&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">) </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">outperforms the best previously reported models (including ensembles) by more than </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> BLEU, establishing a new </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">state-of-the-art BLEU score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. The configuration of this model is listed in the bottom line of Table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Training took </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> days on </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> P100 GPUs. Even our base model surpasses all previously published models and ensembles,</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">at a fraction of the training cost of any of the competitive models. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">On the WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> English-to-French translation task, our big model achieves a BLEU score of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">41.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, outperforming all </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">of the previously published single models, at less than </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> the training cost of the previous state-of-the-art </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, instead of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">For the base models, we used a single model obtained by averaging the last </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> checkpoints, which were written at </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">-minute intervals. For the big models, we averaged the last </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> checkpoints. We used beam search with a beam size </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> and length penalty α = </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. These hyperparameters were chosen after experimentation on the </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">development set. We set the maximum output length during inference to input length + </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, but terminate early when </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">possible [(&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">]. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> summarizes our results and compares our translation quality and training costs to other model </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">architectures from the literature. We estimate the number of floating point operations used to train a model by </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">floating-point capacity of each GPU </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> (&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;).</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mOn the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German translation task, the big transformer model \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mTransformer \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mbig\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m in Table \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39moutperforms the best previously reported models \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mincluding ensembles\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m by more than \u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;39m BLEU, establishing a new \u001b[0m\n",
       "\u001b[1;39mstate-of-the-art BLEU score of \u001b[0m\u001b[1;36m28.4\u001b[0m\u001b[1;39m. The configuration of this model is listed in the bottom line of Table \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mTraining took \u001b[0m\u001b[1;36m3.5\u001b[0m\u001b[1;39m days on \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m P100 GPUs. Even our base model surpasses all previously published models and ensembles,\u001b[0m\n",
       "\u001b[1;39mat a fraction of the training cost of any of the competitive models. \u001b[0m\n",
       "\u001b[1;39mOn the WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;39m English-to-French translation task, our big model achieves a BLEU score of \u001b[0m\u001b[1;36m41.0\u001b[0m\u001b[1;39m, outperforming all \u001b[0m\n",
       "\u001b[1;39mof the previously published single models, at less than \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m/\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m the training cost of the previous state-of-the-art \u001b[0m\n",
       "\u001b[1;39mmodel. The Transformer \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mbig\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m model trained for English-to-French used dropout rate Pdrop = \u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;39m, instead of \u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mFor the base models, we used a single model obtained by averaging the last \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m checkpoints, which were written at \u001b[0m\n",
       "\u001b[1;36m10\u001b[0m\u001b[1;39m-minute intervals. For the big models, we averaged the last \u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;39m checkpoints. We used beam search with a beam size \u001b[0m\n",
       "\u001b[1;39mof \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m and length penalty α = \u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. These hyperparameters were chosen after experimentation on the \u001b[0m\n",
       "\u001b[1;39mdevelopment set. We set the maximum output length during inference to input length + \u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;39m, but terminate early when \u001b[0m\n",
       "\u001b[1;39mpossible \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;39m]\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mTable \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m summarizes our results and compares our translation quality and training costs to other model \u001b[0m\n",
       "\u001b[1;39marchitectures from the literature. We estimate the number of floating point operations used to train a model by \u001b[0m\n",
       "\u001b[1;39mmultiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision \u001b[0m\n",
       "\u001b[1;39mfloating-point capacity of each GPU \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">To evaluate the importance of different components of the Transformer, we varied our base model in different ways, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">measuring the change in performance on English-to-German translation on the development set, newstest2013. We used </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">beam search as described in the previous section, but no checkpoint averaging. We present these results in Table </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(&lt;</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In Table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">the amount of computation constant, as described in Section (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">. While single-head attention is </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> BLEU </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">worse than the best setting, quality also drops off with too many heads. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">5We used values of </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> and </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> TFLOPS for K80, K40, M40 and P100, respectively. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">Table </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|    | Unnamed: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   | N                                         |   dmodel   |   dff   |   h   |   dk   |   dv   | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">P  drop   |   ls   | train steps     |   PPL  (dev)   |   BLEU  (dev)   |   params  ×</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">106</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|---:|:-------------|:------------------------------------------|-----------:|--------:|------:|-------:|-------:|-</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">-----------:|--------:|:----------------|---------------:|----------------:|-----------------:|</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | base         | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                                         |        </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2048</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">  | 100K            |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.92</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">65</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.29</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.9</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | (A)          | nan                                       |        nan |     nan |   nan |      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   |  nan    | nan             |         nan    |             </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">  |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |         </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |      </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.91</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">            |         nan    |           nan   |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.01</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |   nan |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.1</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | (B)          | nan                                       |        nan |     nan |   nan |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.01</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                                         |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6.11</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">36</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                                         |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.19</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">|  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                                         |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.88</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">80</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | (C)          | nan                                       |        </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">256</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     nan |   nan |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.75</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |       </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |     nan |   nan |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.66</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">168</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.12</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">53</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.75</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">90</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.77</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | (D)          | nan                                       |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">             |         nan    |           nan   |             </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.95</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.5</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">         | nan                                       |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |         nan    |           nan   |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.67</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.2</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">  | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.47</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | (E)          | positional embedding instead of sinusoids |        nan |     nan |   nan |    nan |    nan | </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">nan   |  nan    | nan             |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.92</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.7</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           nan    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">| </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> | big          | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">                                         |       </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1024</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4096</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |    nan |    nan | </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |  nan    | 300K            |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4.33</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |            </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26.4</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> |           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">213</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">    |</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">In Table (&lt;&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very</span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">embeddings [(&lt;</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">], and observe nearly identical results to the base model.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmeasuring the change in performance on English-to-German translation on the development set, newstest2013. We used \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mbeam search as described in the previous section, but no checkpoint averaging. We present these results in Table \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m. \u001b[0m\n",
       "\u001b[1;39mIn Table \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m rows \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mA\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m, we vary the number of attention heads and the attention key and value dimensions, keeping \u001b[0m\n",
       "\u001b[1;39mthe amount of computation constant, as described in Section \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3.2\u001b[0m\u001b[1;39m.\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m. While single-head attention is \u001b[0m\u001b[1;36m0.9\u001b[0m\u001b[1;39m BLEU \u001b[0m\n",
       "\u001b[1;39mworse than the best setting, quality also drops off with too many heads. \u001b[0m\n",
       "\u001b[1;39m5We used values of \u001b[0m\u001b[1;36m2.8\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;36m3.7\u001b[0m\u001b[1;39m, \u001b[0m\u001b[1;36m6.0\u001b[0m\u001b[1;39m and \u001b[0m\u001b[1;36m9.5\u001b[0m\u001b[1;39m TFLOPS for K80, K40, M40 and P100, respectively. \u001b[0m\n",
       "\u001b[1;39mTable \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All \u001b[0m\n",
       "\u001b[1;39mmetrics are on the English-to-German translation development set, newstest2013. Listed perplexities are \u001b[0m\n",
       "\u001b[1;39mper-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities. \u001b[0m\n",
       "\u001b[1;39m|    | Unnamed: \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m   | N                                         |   dmodel   |   dff   |   h   |   dk   |   dv   | \u001b[0m\n",
       "\u001b[1;39mP  drop   |   ls   | train steps     |   PPL  \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mdev\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m   |   BLEU  \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mdev\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m   |   params  ×\u001b[0m\u001b[1;36m106\u001b[0m\u001b[1;39m   |\u001b[0m\n",
       "\u001b[1;39m|---:|:-------------|:------------------------------------------|-----------:|--------:|------:|-------:|-------:|-\u001b[0m\n",
       "\u001b[1;39m-----------:|--------:|:----------------|---------------:|----------------:|-----------------:|\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m | base         | \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;39m                                         |        \u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;36m0.1\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;39m  | 100K            |           \u001b[0m\u001b[1;36m4.92\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.8\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m65\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |     \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.29\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m24.9\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mA\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m          | nan                                       |        nan |     nan |   nan |      \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;36m128\u001b[0m\u001b[1;39m   |  nan    | nan             |         nan    |             \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m   |            \u001b[0m\u001b[1;36m25.5\u001b[0m\u001b[1;39m  |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m | nan          | nan                                       |         \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m |      \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |    \u001b[0m\u001b[1;36m4.91\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;36m25.8\u001b[0m\u001b[1;39m            |         nan    |           nan   |           nan    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |    \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.01\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.4\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |   nan |     \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.16\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.1\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m58\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mB\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m          | nan                                       |        nan |     nan |   nan |     \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.01\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.4\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;39m | nan          | \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;39m                                         |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m6.11\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m23.7\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m36\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m | nan          | \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;39m                                         |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.19\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.3\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m|  \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;39m | nan          | \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;39m                                         |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m4.88\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.5\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m80\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mC\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m          | nan                                       |        \u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;39m |     nan |   nan |     \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m |     \u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.75\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m24.5\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;39m | nan          | nan                                       |       \u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;39m |     nan |   nan |    \u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;39m | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m4.66\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;39m   |           \u001b[0m\u001b[1;36m168\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |    \u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;39m |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.12\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.4\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m53\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |    \u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;39m |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m4.75\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m26.2\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;36m0\u001b[0m\u001b[1;39m   |  nan    | nan             |           \u001b[0m\u001b[1;36m5.77\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m24.6\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mD\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m          | nan                                       |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | \u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;39m             |         nan    |           nan   |             \u001b[0m\u001b[1;36m4.95\u001b[0m\u001b[1;39m |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;36m25.5\u001b[0m\u001b[1;39m         | nan                                       |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |         nan    |           nan   |           nan    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |    \u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;39m    | nan             |           \u001b[0m\u001b[1;36m4.67\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.3\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;39m | nan          | nan                                       |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |    \u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;39m  | nan             |           \u001b[0m\u001b[1;36m5.47\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.7\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;39m | \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mE\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m          | positional embedding instead of sinusoids |        nan |     nan |   nan |    nan |    nan | \u001b[0m\n",
       "\u001b[1;39mnan   |  nan    | nan             |           \u001b[0m\u001b[1;36m4.92\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m25.7\u001b[0m\u001b[1;39m |           nan    |\u001b[0m\n",
       "\u001b[1;39m| \u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;39m | big          | \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;39m                                         |       \u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m4096\u001b[0m\u001b[1;39m |    \u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;39m |    nan |    nan | \u001b[0m\n",
       "\u001b[1;36m0.3\u001b[0m\u001b[1;39m |  nan    | 300K            |           \u001b[0m\u001b[1;36m4.33\u001b[0m\u001b[1;39m |            \u001b[0m\u001b[1;36m26.4\u001b[0m\u001b[1;39m |           \u001b[0m\u001b[1;36m213\u001b[0m\u001b[1;39m    |\u001b[0m\n",
       "\u001b[1;39mIn Table \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;39m rows \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mB\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m, we observe that reducing the attention key size dk hurts model quality. This suggests that\u001b[0m\n",
       "\u001b[1;39mdetermining compatibility is not easy and that a more sophisticated compatibility function than dot product may be \u001b[0m\n",
       "\u001b[1;39mbeneficial. We further observe in rows \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mC\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m and \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mD\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m that, as expected, bigger models are better, and dropout is very\u001b[0m\n",
       "\u001b[1;39mhelpful in avoiding over-fitting. In row \u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39mE\u001b[0m\u001b[1;39m)\u001b[0m\u001b[1;39m we replace our sinusoidal positional encoding with learned positional \u001b[0m\n",
       "\u001b[1;39membeddings \u001b[0m\u001b[1;39m[\u001b[0m\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, and observe nearly identical results to the base model.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">self-attention. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">convolutional layers. On both WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-German and WMT </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> English-to-French translation tasks, we </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">achieve a new state of the art. In the former task our best model outperforms even all previously reported </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">ensembles. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">the Transformer to problems involving input and output modalities other than text and to investigate local, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Making generation less sequential is another research goals of ours. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The code we used to train and evaluate our models is available at </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(&lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/tensorflow/tensor2tensor</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">&gt;)</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/</span><span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\"> </span>\n",
       "<span style=\"color: #000000; text-decoration-color: #000000; font-weight: bold\">(&lt;</span><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://github.com/tensorflow/tensor2tensor</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">&gt;)tensorflow/tensor2tensor. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inspiration.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mreplacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mself-attention. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mconvolutional layers. On both WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-German and WMT \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m English-to-French translation tasks, we \u001b[0m\n",
       "\u001b[1;38;2;118;185;0machieve a new state of the art. In the former task our best model outperforms even all previously reported \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mensembles. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mthe Transformer to problems involving input and output modalities other than text and to investigate local, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrestricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mMaking generation less sequential is another research goals of ours. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mThe code we used to train and evaluate our models is available at \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0m<\u001b[0m\u001b[4;94mhttps:\u001b[0m\u001b[4;94m//github.com/tensorflow/tensor2tensor\u001b[0m\u001b[1;39m>\u001b[0m\u001b[1;39m)\u001b[0m\u001b[4;94mhttps://github.com/\u001b[0m\u001b[1;39m \u001b[0m\n",
       "\u001b[1;39m(\u001b[0m\u001b[1;39m<\u001b[0m\u001b[4;94mhttps://github.com/tensorflow/tensor2tensor\u001b[0m\u001b[1;38;2;118;185;0m>\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0mtensorflow/tensor2tensor. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0minspiration.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1607.06450</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and translate. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1409.0473</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">architectures. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1703.03906</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1601.06733</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Learning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1406.1078</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1610.02357</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">neural networks on sequence modeling. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1412.3555</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-tional sequence to </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sequence learning. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1705.</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">03122v2, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1308.0850</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2013</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">770</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">–</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">778</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">11</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">difficulty of learning long-term dependencies, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2001</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">):</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1735</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">–</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1780</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1997</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">language modeling. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1602.02410</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Representations (ICLR), </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-ray Kavukcuoglu. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Neural machine translation in linear time. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1610.</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">10099v2, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">16</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Conference on Learning Representations, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1703.10722</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">structured self-attentive sentence embedding. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1703.03130</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Systems, (NIPS), </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">machine translation. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1508.04025</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">22</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Empirical Methods in Natural Language Processing, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1705.04304</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1608.05859</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">units. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1508.07909</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1701.06538</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2017</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">27</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov. Dropout: a </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">15</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">):</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1929</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">–</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1958</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems </span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, pages </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2440</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">–</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2448</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. Curran Associates, Inc., </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">29</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Advances in Neural Information Processing Systems, pages </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3104</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">–</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3112</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2014</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">30</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">inception architecture for computer vision. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1512.00567</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2015</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">machine translation. arXiv preprint arXiv:</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1609.08144</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">. </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">]  Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">for neural machine translation. CoRR, abs/</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1606.04199</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2016</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1607.06450\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand translate. CoRR, abs/\u001b[0m\u001b[1;36m1409.0473\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marchitectures. CoRR, abs/\u001b[0m\u001b[1;36m1703.03906\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mpreprint arXiv:\u001b[0m\u001b[1;36m1601.06733\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mLearning phrase representations using rnn encoder-decoder for statistical machine translation. CoRR, abs/\u001b[0m\u001b[1;36m1406.1078\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv preprint \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marXiv:\u001b[0m\u001b[1;36m1610.02357\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mneural networks on sequence modeling. CoRR, abs/\u001b[0m\u001b[1;36m1412.3555\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-tional sequence to \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msequence learning. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1705.\u001b[0m\u001b[1;38;2;118;185;0m03122v2, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1308.0850\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2013\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages \u001b[0m\u001b[1;36m770\u001b[0m\u001b[1;38;2;118;185;0m–\u001b[0m\u001b[1;36m778\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in recurrent nets: the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mdifficulty of learning long-term dependencies, \u001b[0m\u001b[1;36m2001\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, \u001b[0m\u001b[1;35m9\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m:\u001b[0m\u001b[1;36m1735\u001b[0m\u001b[1;38;2;118;185;0m–\u001b[0m\u001b[1;36m1780\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m1997\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mlanguage modeling. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1602.02410\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m14\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference on Learning \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mRepresentations \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mICLR\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-ray Kavukcuoglu. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mNeural machine translation in linear time. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1610.\u001b[0m\u001b[1;38;2;118;185;0m10099v2, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mConference on Learning Representations, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, \u001b[0m\u001b[1;36m2015\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1703.10722\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
       "\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. A \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mstructured self-attentive sentence embedding. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1703.03130\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural Information Processing \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mSystems, \u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;38;2;118;185;0mNIPS\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-based neural \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmachine translation. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1508.04025\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2015\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m22\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention model. In \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mEmpirical Methods in Natural Language Processing, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m23\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive summarization. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marXiv preprint arXiv:\u001b[0m\u001b[1;36m1705.04304\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint \u001b[0m\n",
       "\u001b[1;38;2;118;185;0marXiv:\u001b[0m\u001b[1;36m1608.05859\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword \u001b[0m\n",
       "\u001b[1;38;2;118;185;0munits. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1508.07909\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2015\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m26\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mOutrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1701.06538\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;36m2017\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov. Dropout: a \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msimple way to prevent neural networks from overfitting. Journal of Machine Learning Research, \u001b[0m\u001b[1;35m15\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m:\u001b[0m\u001b[1;36m1929\u001b[0m\u001b[1;38;2;118;185;0m–\u001b[0m\u001b[1;36m1958\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
       "\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mN. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems \u001b[0m\n",
       "\u001b[1;36m28\u001b[0m\u001b[1;38;2;118;185;0m, pages \u001b[0m\u001b[1;36m2440\u001b[0m\u001b[1;38;2;118;185;0m–\u001b[0m\u001b[1;36m2448\u001b[0m\u001b[1;38;2;118;185;0m. Curran Associates, Inc., \u001b[0m\u001b[1;36m2015\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m29\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mAdvances in Neural Information Processing Systems, pages \u001b[0m\u001b[1;36m3104\u001b[0m\u001b[1;38;2;118;185;0m–\u001b[0m\u001b[1;36m3112\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2014\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minception architecture for computer vision. CoRR, abs/\u001b[0m\u001b[1;36m1512.00567\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2015\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m31\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mCao, Qin Gao, Klaus Macherey, et al. Google’s neural machine translation system: Bridging the gap between human and\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mmachine translation. arXiv preprint arXiv:\u001b[0m\u001b[1;36m1609.08144\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m. \u001b[0m\n",
       "\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m  Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward connections \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mfor neural machine translation. CoRR, abs/\u001b[0m\u001b[1;36m1606.04199\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;36m2016\u001b[0m\u001b[1;38;2;118;185;0m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    pprint(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb = OpenAIEmbeddings()\n",
    "emb = NVIDIAEmbeddings(model=\"nvolveqa_40k\")\n",
    "vdb = DocArrayInMemorySearch.from_documents(chunks, emb)\n",
    "\n",
    "qas = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vdb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">The Transformer model you've described uses a multi-head self-attention mechanism. In this mechanism, keys, values,</span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">and queries all come from the same place in the model, allowing each position to attend to all positions in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">same sequence. This is different from encoder-decoder attention, where queries come from the decoder and keys and </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">values come from the encoder output, allowing every position in the decoder to attend over all positions in the </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">input sequence. Multi-head attention is used in three different ways in the Transformer model: encoder-decoder </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">attention, encoder self-attention, and decoder self-attention. The self-attention mechanism is used to compute </span>\n",
       "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">representations of the input and output sequences without using sequence-aligned RNNs or convolution.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;118;185;0mThe Transformer model you've described uses a multi-head self-attention mechanism. In this mechanism, keys, values,\u001b[0m\n",
       "\u001b[1;38;2;118;185;0mand queries all come from the same place in the model, allowing each position to attend to all positions in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0msame sequence. This is different from encoder-decoder attention, where queries come from the decoder and keys and \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mvalues come from the encoder output, allowing every position in the decoder to attend over all positions in the \u001b[0m\n",
       "\u001b[1;38;2;118;185;0minput sequence. Multi-head attention is used in three different ways in the Transformer model: encoder-decoder \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mattention, encoder self-attention, and decoder self-attention. The self-attention mechanism is used to compute \u001b[0m\n",
       "\u001b[1;38;2;118;185;0mrepresentations of the input and output sequences without using sequence-aligned RNNs or convolution.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# question = \"What is the author list?\"\n",
    "# question = \"What is the title of the paper?\"\n",
    "question = \"What is the attention mechanism used in the model?\"\n",
    "response = qas({\"query\": question})\n",
    "pprint(response[\"result\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
